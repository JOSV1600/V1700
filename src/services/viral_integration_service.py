# services/viral_integration_service.py
"""VIRAL IMAGE FINDER - ARQV30 Enhanced v3.0
M√≥dulo para buscar imagens virais no Google Imagens de Instagram/Facebook
Analisa engajamento, extrai links dos posts e salva dados estruturados
CORRIGIDO: APIs funcionais, extra√ß√£o real de imagens, fallbacks robustos
"""
import os
import re
import json
import time
import asyncio
import logging
import ssl
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse, parse_qs, unquote, urljoin
from dataclasses import dataclass, asdict
import hashlib
import requests

# Import condicional do aiohttp, caso esteja dispon√≠vel
try:
    import aiohttp
    import aiofiles
    HAS_ASYNC_DEPS = True
except ImportError:
    HAS_ASYNC_DEPS = False
    logger = logging.getLogger(__name__)
    logger.warning("aiohttp/aiofiles n√£o encontrados. Usando requests s√≠ncrono como fallback.")


# Configura√ß√£o de logging
logger = logging.getLogger(__name__)

@dataclass
class ViralImage:
    """Estrutura de dados para imagem viral"""
    image_url: str
    post_url: str
    platform: str
    title: str
    description: str
    engagement_score: float
    views_estimate: int
    likes_estimate: int
    comments_estimate: int
    shares_estimate: int
    author: str
    author_followers: int
    post_date: str
    hashtags: List[str]
    image_path: Optional[str] = None
    screenshot_path: Optional[str] = None
    extracted_at: str = datetime.now().isoformat()

class ViralImageFinder:
    """Classe principal para encontrar imagens virais"""

    def __init__(self, config: Dict = None):
        self.config = config or self._load_config()
        self.api_keys = self._load_multiple_api_keys()
        self.current_api_index = {
            'serper': 0,
            'google_cse': 0
        }
        self.failed_apis = set()
        self._ensure_directories()
        self.session = requests.Session()
        self.setup_session()

    def _load_config(self) -> Dict:
        """Carrega configura√ß√µes do ambiente"""
        return {
            'serper_api_key': os.getenv('SERPER_API_KEY'),
            'google_search_key': os.getenv('GOOGLE_SEARCH_KEY'),
            'google_cse_id': os.getenv('GOOGLE_CSE_ID'),
            'max_images': int(os.getenv('MAX_IMAGES', 30)),
            'timeout': int(os.getenv('TIMEOUT', 30)),
            'output_dir': os.getenv('OUTPUT_DIR', 'viral_images_data'),
            'images_dir': os.getenv('IMAGES_DIR', 'downloaded_images'),
            'extract_images': os.getenv('EXTRACT_IMAGES', 'True').lower() == 'true',
        }

    def _load_multiple_api_keys(self) -> Dict:
        """Carrega m√∫ltiplas chaves de API para rota√ß√£o"""
        api_keys = {
            'serper': [],
            'google_cse': []
        }

        # Serper - m√∫ltiplas chaves
        for i in range(1, 5): # Tenta carregar at√© 4 chaves Serper
            key = os.getenv(f'SERPER_API_KEY_{i}') or (os.getenv('SERPER_API_KEY') if i == 1 else None)
            if key and key.strip():
                api_keys['serper'].append(key.strip())
                logger.info(f"‚úÖ Serper API {i} carregada")

        # Google CSE
        google_key = os.getenv('GOOGLE_SEARCH_KEY')
        google_cse = os.getenv('GOOGLE_CSE_ID')
        if google_key and google_cse:
            api_keys['google_cse'].append({'key': google_key, 'cse_id': google_cse})
            logger.info(f"‚úÖ Google CSE carregada")

        return api_keys

    def _get_next_api_key(self, service: str) -> Optional[str]:
        """Obt√©m pr√≥xima chave de API dispon√≠vel com rota√ß√£o autom√°tica"""
        if service not in self.api_keys or not self.api_keys[service]:
            return None

        keys = self.api_keys[service]
        if not keys:
            return None

        # Tenta todas as chaves dispon√≠veis
        for attempt in range(len(keys)):
            current_index = self.current_api_index[service]
            # Verifica se esta API n√£o falhou recentemente (implementa√ß√£o simples, sem fallback autom√°tico)
            api_identifier = f"{service}_{current_index}"
            if api_identifier not in self.failed_apis:
                key = keys[current_index]
                logger.info(f"üîÑ Usando {service} API #{current_index + 1}")
                # Avan√ßa para a pr√≥xima API na pr√≥xima chamada
                self.current_api_index[service] = (current_index + 1) % len(keys)
                return key
            # Se esta API falhou, tenta a pr√≥xima
            self.current_api_index[service] = (current_index + 1) % len(keys)
        
        logger.error(f"‚ùå Todas as APIs de {service} falharam recentemente")
        return None


    def _ensure_directories(self):
        """Garante que todos os diret√≥rios necess√°rios existam"""
        dirs_to_create = [
            self.config['output_dir'],
            self.config['images_dir']
        ]
        for directory in dirs_to_create:
            try:
                os.makedirs(directory, exist_ok=True)
                logger.info(f"‚úÖ Diret√≥rio criado/verificado: {directory}")
            except Exception as e:
                logger.error(f"‚ùå Erro ao criar diret√≥rio {directory}: {e}")

    def setup_session(self):
        """Configura sess√£o HTTP com headers apropriados"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })

    async def search_images(self, query: str) -> List[Dict]:
        """Busca imagens usando Google Images via Serper com queries otimizadas"""
        logger.info(f"üîç INICIANDO BUSCA DE IMAGENS: {query}")

        all_results = []

        # Queries espec√≠ficas para redes sociais e conte√∫do educacional
        queries = [
            f'"{query}" site:instagram.com',
            f'site:instagram.com/p "{query}"',
            f'site:instagram.com/reel "{query}"',
            f'"{query}" site:facebook.com',
            f'"{query}" site:youtube.com',
            f'"{query}" curso online tutorial'
        ]

        for q in queries[:4]:  # Limitar a 4 queries para controle de rate limit
            logger.info(f"üîç Buscando imagens para: {q}")
            results = await self._search_serper_images(q)
            all_results.extend(results)
            await asyncio.sleep(0.5)  # Pequena pausa entre as buscas

        # Remove duplicatas baseadas na URL da imagem
        seen_image_urls = set()
        unique_results = []
        for result in all_results:
            image_url = result.get('image_url', '')
            if image_url and image_url not in seen_image_urls:
                seen_image_urls.add(image_url)
                unique_results.append(result)

        logger.info(f"‚úÖ Total de imagens √∫nicas encontradas: {len(unique_results)}")
        return unique_results[:self.config['max_images']]

    async def _search_serper_images(self, query: str) -> List[Dict]:
        """Busca imagens usando Serper API com rota√ß√£o de chaves e fallbacks"""
        api_key = self._get_next_api_key('serper')
        if not api_key:
            logger.error("‚ùå Nenhuma API Serper dispon√≠vel para esta busca.")
            return []

        url = "https://google.serper.dev/images"
        payload = {
            "q": query,
            "num": 20, # Quantidade de resultados por p√°gina
            "safe": "off", # Desabilita busca segura
            "gl": "br", # Pa√≠s para a busca
            "hl": "pt-br", # Idioma da busca
            "imgSize": "large", # Tamanho da imagem
            "imgType": "photo" # Tipo de imagem
        }

        headers = {
            'X-API-KEY': api_key,
            'Content-Type': 'application/json'
        }

        try:
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=self.config['timeout'])
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.post(url, headers=headers, json=payload) as response:
                        response.raise_for_status() # Lan√ßa exce√ß√£o para status de erro
                        data = await response.json()
            else:
                response = self.session.post(url, headers=headers, json=payload, timeout=self.config['timeout'])
                response.raise_for_status()
                data = response.json()

            results = []
            for item in data.get('images', []):
                image_url = item.get('imageUrl', '')
                # Valida a URL da imagem e calcula um score b√°sico de viralidade
                if image_url and self._is_valid_image_url(image_url):
                    results.append({
                        'image_url': image_url,
                        'page_url': item.get('link', ''), # URL da p√°gina onde a imagem foi encontrada
                        'title': item.get('title', ''), # T√≠tulo do resultado
                        'description': item.get('snippet', ''), # Descri√ß√£o do resultado
                        'source': 'serper_images', # Fonte dos dados
                        'platform': self._detect_platform(item.get('link', '')), # Plataforma detectada
                        'viral_score': self._calculate_viral_score(item) # Score de viralidade
                    })

            logger.info(f"üìä Serper encontrou {len(results)} imagens v√°lidas para '{query}'")
            return results

        except Exception as e:
            # Marca a API como falha se ocorrer um erro
            current_index = (self.current_api_index["serper"] - 1) % len(self.api_keys["serper"]) if self.api_keys.get("serper") else 0
            # self._mark_api_failed("serper", current_index) # Implementa√ß√£o de marca√ß√£o de falha pode ser adicionada aqui
            logger.error(f"‚ùå Erro na busca Serper para '{query}': {e}")
            return []

    def _is_valid_image_url(self, url: str) -> bool:
        """Verifica se a URL parece ser de uma imagem real, evitando p√°ginas de login ou n√£o-imagem."""
        if not url or not isinstance(url, str):
            return False

        # Padr√µes para URLs que claramente N√ÉO s√£o imagens
        invalid_patterns = [
            r'instagram\.com/accounts/login', # P√°ginas de login do Instagram
            r'facebook\.com/login',           # P√°ginas de login do Facebook
            r'login\.php',                    # Scripts de login gen√©ricos
            r'/login/',                       # Padr√£o comum para URLs de login
            r'/auth/',                        # Padr√£o comum para URLs de autentica√ß√£o
            r'\.html$',                       # Arquivos HTML geralmente n√£o s√£o imagens diretas
            r'\.php$',                        # Arquivos PHP geralmente n√£o s√£o imagens diretas
            r'\.jsp$',                        # Arquivos JSP geralmente n√£o s√£o imagens diretas
            r'\.asp$'                         # Arquivos ASP geralmente n√£o s√£o imagens diretas
        ]

        # Verifica se alguma URL inv√°lida est√° presente
        if any(re.search(pattern, url, re.IGNORECASE) for pattern in invalid_patterns):
            return False

        # Padr√µes para URLs que PROVAVELMENTE s√£o imagens
        valid_patterns = [
            r'\.(jpg|jpeg|png|gif|webp|bmp)(\?|$)', # Extens√µes comuns de imagem com ou sem query params
            r'scontent.*\.(jpg|png|webp)',        # URLs de CDN do Instagram
            r'cdninstagram\.com',                 # Outra forma de CDN do Instagram
            r'fbcdn\.net',                        # URLs de CDN do Facebook
            r'img\.youtube\.com',                 # Thumbnails do YouTube
            r'i\.ytimg\.com',                     # Thumbnails alternativos do YouTube
            r'googleusercontent\.com',            # Imagens do Google
            r'ggpht\.com',                        # Google Photos/YouTube
            r'licdn\.com',                        # CDN do LinkedIn
            r'linkedin\.com.*\.(jpg|png|webp)',  # Imagens do LinkedIn
            r'scontent-.*\.cdninstagram\.com',     # CDN espec√≠fico do Instagram
            r'scontent\..*\.fbcdn\.net'          # CDN espec√≠fico do Facebook
        ]

        # Verifica se alguma URL v√°lida est√° presente
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in valid_patterns)

    def _detect_platform(self, url: str) -> str:
        """Detecta a plataforma (Instagram, Facebook, YouTube, etc.) baseada na URL."""
        if 'instagram.com' in url:
            return 'instagram'
        elif 'facebook.com' in url:
            return 'facebook'
        elif 'youtube.com' in url or 'youtu.be' in url:
            return 'youtube'
        elif 'linkedin.com' in url:
            return 'linkedin'
        else:
            return 'web' # Plataforma gen√©rica se n√£o for reconhecida

    def _calculate_viral_score(self, item: Dict) -> float:
        """Calcula um score de viralidade b√°sico baseado em metadados do Serper."""
        score = 5.0  # Score base inicial

        # Fatores que aumentam o score
        title = item.get('title', '').lower()

        # Palavras-chave que sugerem viralidade/popularidade
        viral_keywords = ['viral', 'trending', 'popular', 'mil', 'views', 'likes', 'compartilh', 'sucesso']
        for keyword in viral_keywords:
            if keyword in title:
                score += 1.0 # Adiciona 1 ponto para cada palavra-chave encontrada

        # Plataformas sociais (Instagram, Facebook, YouTube) tendem a ter mais engajamento viral
        link = item.get('link', '')
        if any(platform in link for platform in ['instagram.com', 'facebook.com', 'youtube.com']):
            score += 2.0 # Adiciona 2 pontos para plataformas sociais

        return min(score, 10.0)  # Limita o score m√°ximo a 10.0

    async def extract_images_with_content(self, query: str) -> List[ViralImage]:
        """Fun√ß√£o principal para buscar imagens virais e baixar o conte√∫do."""
        logger.info(f"üöÄ EXTRAINDO IMAGENS COM CONTE√öDO para a consulta: {query}")

        # Realiza a busca inicial de imagens usando Serper
        image_results = await self.search_images(query)

        viral_images = []
        # Processa cada resultado da busca
        for i, result in enumerate(image_results):
            try:
                # Cria um objeto ViralImage com os dados extra√≠dos
                viral_image = ViralImage(
                    image_url=result['image_url'], # URL da imagem
                    post_url=result.get('page_url', ''), # URL do post/p√°gina
                    platform=result.get('platform', 'web'), # Plataforma detectada
                    title=result.get('title', ''), # T√≠tulo do resultado
                    description=result.get('description', ''), # Descri√ß√£o do resultado
                    engagement_score=result.get('viral_score', 5.0), # Score de viralidade
                    views_estimate=0, # Placeholder, pode ser preenchido posteriormente
                    likes_estimate=0, # Placeholder
                    comments_estimate=0, # Placeholder
                    shares_estimate=0, # Placeholder
                    author='', # Placeholder
                    author_followers=0, # Placeholder
                    post_date='', # Placeholder
                    hashtags=[] # Placeholder
                )

                # Baixa a imagem se a configura√ß√£o permitir
                if self.config['extract_images']:
                    # Gera um nome de arquivo √∫nico baseado no √≠ndice e na URL
                    image_filename = f"viral_image_{i+1}_{hashlib.md5(result['image_url'].encode()).hexdigest()[:6]}"
                    image_path = await self._download_image(result['image_url'], image_filename)
                    viral_image.image_path = image_path # Define o caminho do arquivo baixado

                viral_images.append(viral_image) # Adiciona a imagem processada √† lista

            except Exception as e:
                # Registra erro se o processamento de uma imagem falhar
                logger.error(f"‚ùå Erro ao processar imagem {i+1} (URL: {result.get('image_url')}): {e}")
                continue # Continua para a pr√≥xima imagem

        logger.info(f"‚úÖ {len(viral_images)} imagens virais extra√≠das com sucesso para a consulta '{query}'")
        return viral_images

    async def _download_image(self, image_url: str, filename: str) -> Optional[str]:
        """Baixa uma imagem da URL de forma robusta, usando aiohttp se dispon√≠vel."""
        try:
            # Define headers para simular um navegador
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.google.com/' # Define Referer para evitar bloqueios
            }

            # Utiliza aiohttp para downloads ass√≠ncronos se as depend√™ncias estiverem instaladas
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=self.config['timeout'])
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(image_url, headers=headers) as response:
                        response.raise_for_status() # Verifica se a requisi√ß√£o foi bem-sucedida

                        # Obt√©m o tipo de conte√∫do para validar se √© uma imagem
                        content_type = response.headers.get('content-type', '').lower()
                        if 'image' not in content_type:
                            logger.warning(f"URL n√£o √© uma imagem v√°lida: {image_url} (Content-Type: {content_type})")
                            return None

                        # Determina a extens√£o do arquivo com base no Content-Type
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            ext = '.jpg'
                        elif 'png' in content_type:
                            ext = '.png'
                        elif 'webp' in content_type:
                            ext = '.webp'
                        else:
                            ext = '.jpg'  # Extens√£o padr√£o se n√£o for reconhecida

                        # Cria o caminho completo para salvar o arquivo
                        filepath = os.path.join(self.config['images_dir'], f"{filename}{ext}")

                        # Salva o conte√∫do da imagem em chunks
                        async with aiofiles.open(filepath, 'wb') as f:
                            async for chunk in response.content.iter_chunked(8192): # L√™ em blocos de 8KB
                                await f.write(chunk)

                        # Verifica se o arquivo foi salvo corretamente (tamanho m√≠nimo)
                        if os.path.exists(filepath) and os.path.getsize(filepath) > 1000:  # Verifica se tem pelo menos 1KB
                            logger.info(f"‚úÖ Imagem baixada com sucesso: {filepath}")
                            return filepath
                        else:
                            # Remove arquivo inv√°lido se existir
                            if os.path.exists(filepath):
                                os.remove(filepath)
                            logger.warning(f"Arquivo de imagem inv√°lido ou vazio foi criado: {filepath}")
                            return None
            else: # Fallback para requests s√≠ncrono se aiohttp n√£o estiver dispon√≠vel
                response = self.session.get(image_url, headers=headers, timeout=self.config['timeout'])
                response.raise_for_status()

                content_type = response.headers.get('content-type', '').lower()
                if 'image' not in content_type:
                    logger.warning(f"URL n√£o √© uma imagem v√°lida: {image_url} (Content-Type: {content_type})")
                    return None

                if 'jpeg' in content_type or 'jpg' in content_type: ext = '.jpg'
                elif 'png' in content_type: ext = '.png'
                elif 'webp' in content_type: ext = '.webp'
                else: ext = '.jpg'

                filepath = os.path.join(self.config['images_dir'], f"{filename}{ext}")

                with open(filepath, 'wb') as f:
                    f.write(response.content)

                if os.path.exists(filepath) and os.path.getsize(filepath) > 1000:
                    logger.info(f"‚úÖ Imagem baixada com sucesso: {filepath}")
                    return filepath
                else:
                    if os.path.exists(filepath):
                        os.remove(filepath)
                    logger.warning(f"Arquivo de imagem inv√°lido ou vazio foi criado: {filepath}")
                    return None

        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar imagem de {image_url}: {e}")
            return None

    def save_results(self, viral_images: List[ViralImage], filename: str = None) -> str:
        """Salva a lista de imagens virais em um arquivo JSON."""
        # Define um nome de arquivo padr√£o se nenhum for fornecido
        if not filename:
            filename = f"viral_images_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # Cria o caminho completo para o arquivo de resultados
        filepath = os.path.join(self.config['output_dir'], filename)

        # Estrutura os dados para salvar em JSON
        data = {
            'timestamp': datetime.now().isoformat(), # Data e hora da extra√ß√£o
            'total_images': len(viral_images), # N√∫mero total de imagens encontradas
            'images': [asdict(img) for img in viral_images] # Lista de imagens convertidas para dicion√°rio
        }

        # Salva os dados em um arquivo JSON com codifica√ß√£o UTF-8 e indenta√ß√£o
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ Resultados salvos com sucesso em: {filepath}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar resultados em {filepath}: {e}")

        return filepath

# Cria uma inst√¢ncia global do servi√ßo para uso
viral_image_finder = ViralImageFinder()

# Fun√ß√µes wrapper para compatibilidade com c√≥digo existente, se necess√°rio
async def find_viral_images_async(query: str) -> List[ViralImage]:
    """Wrapper ass√≠ncrono para a fun√ß√£o principal."""
    return await viral_image_finder.extract_images_with_content(query)

def find_viral_images_sync(query: str) -> List[ViralImage]:
    """Wrapper s√≠ncrono que executa a fun√ß√£o ass√≠ncrona em um loop de eventos."""
    if HAS_ASYNC_DEPS:
        try:
            # Tenta obter o loop de eventos atual
            loop = asyncio.get_running_loop()
            # Se um loop j√° estiver rodando, executa a tarefa em uma thread separada
            import concurrent.futures
            def run_async_task():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(viral_image_finder.extract_images_with_content(query))
                finally:
                    new_loop.close()
            # Utiliza ThreadPoolExecutor para executar a tarefa ass√≠ncrona
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_async_task)
                return future.result(timeout=300)  # Timeout de 5 minutos
        except RuntimeError:
            # Se n√£o houver loop de eventos rodando, cria um novo
            return asyncio.run(viral_image_finder.extract_images_with_content(query))
    else:
        # Se aiohttp n√£o estiver dispon√≠vel, executa a l√≥gica s√≠ncrona diretamente (com fallback para requests)
        # A l√≥gica s√≠ncrona j√° est√° dentro de _search_serper_images e _download_image
        # Precisamos simular a chamada ass√≠ncrona
        logger.warning("aiohttp n√£o dispon√≠vel, executando busca de forma s√≠ncrona com fallback.")
        # Note: A adapta√ß√£o completa para s√≠ncrono exigiria reescrever partes do c√≥digo.
        # Por ora, retornamos uma lista vazia e um log de aviso.
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(viral_image_finder.extract_images_with_content(query))
        loop.close()
        return result


logger.info("üî• Viral Integration Service (Vers√£o Otimizada) inicializado.")